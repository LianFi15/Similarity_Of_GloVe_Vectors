{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7fT_576DVod"
      },
      "source": [
        "## Word embeddings\n",
        "\n",
        "The below code and text are for the second problem on the pset.  Note that the second code chunk will take several minutes to run, but only needs to be run once, which will download the GLoVe vectors and save them on your Google drive in a new folder named *096222-pset-3* (about 1GB for the glove.6B.zip dataset). When done with the pset you may delete the files to free up space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-vyz0iEETfC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0da17cf6-f7bc-403d-9352-4f890e410213"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "GDRIVE_DIR = \"/content/gdrive/My Drive/096222-pset-3\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3Q3B1NvEYbp",
        "outputId": "d074dc79-0ee0-4a73-ac29-c7312186b4f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# This code chunk needs to be run only the first time through the pset.\n",
        "# It downloads the GLoVe word embeddings and saves them to your Google drive.\n",
        "!time wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "!mkdir -p \"$GDRIVE_DIR\"\n",
        "!mv glove.6B.300d.txt \"$GDRIVE_DIR/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-11 14:02:00--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-05-11 14:02:01--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-05-11 14:02:01--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip          5%[>                   ]  41.93M  4.99MB/s    eta 77s    ^C\n",
            "Archive:  glove.6B.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of glove.6B.zip or\n",
            "        glove.6B.zip.zip, and cannot find glove.6B.zip.ZIP, period.\n",
            "mv: cannot stat 'glove.6B.300d.txt': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-TOpPuEFz88"
      },
      "source": [
        "import sys\n",
        "import numpy\n",
        "\n",
        "def read_vectors_from_file(filename):\n",
        "    d = {}\n",
        "    with open(filename, 'rt') as infile:\n",
        "        for line in infile:\n",
        "            word, *rest = line.split()\n",
        "            d[word] = numpy.array(list(map(float, rest)))\n",
        "    return d\n",
        "\n",
        "e = read_vectors_from_file(GDRIVE_DIR + \"/glove.6B.300d.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LesBcv53ffW"
      },
      "source": [
        "e['apples']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2cpFlNf40bv"
      },
      "source": [
        "### Implement and test the cosine measure of word similarity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "3tDkj-ysWjFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz9GTey-2xxl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e023bf46-52c4-4ef3-f96d-c126f70e72cf"
      },
      "source": [
        "## Write a function to compute the cosine similarity between two word vectors.\n",
        "##       Demonstrate that it's symmetric with a few examples.\n",
        "def cosine_similarity(x: np.ndarray, y: np.ndarray) -> float:\n",
        "    dot_product = np.dot (x,y)\n",
        "    x_norm = np.linalg.norm(x)\n",
        "    y_norm = np.linalg.norm(y)\n",
        "\n",
        "    return dot_product/ (x_norm* y_norm)\n",
        "\n",
        "\n",
        "def verify(x):\n",
        "  if x:\n",
        "    print(\"Verified\")\n",
        "  else:\n",
        "    print(\"Failure to verify\")\n",
        "\n",
        "## Use some examples to demonstrate symmetry of your implementation.\n",
        "verify(cosine_similarity(e['apples'],e['oranges'])==cosine_similarity(e['oranges'],e['apples']))\n",
        "\n",
        "## TODO: add a few more examples here.\n",
        "verify(cosine_similarity(e['car'],e['truck'])==cosine_similarity(e['truck'],e['car']))\n",
        "verify(cosine_similarity(e['pen'],e['paper'])==cosine_similarity(e['paper'],e['pen']))\n",
        "verify(cosine_similarity(e['warm'],e['hot'])==cosine_similarity(e['hot'],e['warm']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verified\n",
            "Verified\n",
            "Verified\n",
            "Verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVUx_vL327YX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b99131bd-1129-44c7-e31d-74b379894d71"
      },
      "source": [
        "## Verify the sanity checks in part 1b of the pset PDF.\n",
        "verify(cosine_similarity(e['car'],e['truck']) > cosine_similarity(e['car'],e['person']))\n",
        "verify(cosine_similarity(e['mars'],e['venus']) > cosine_similarity(e['mars'],e['goes'])) # TODO Convert Mars and Venus to lowercase\n",
        "verify(cosine_similarity(e['warm'],e['cool']) > cosine_similarity(e['warm'],e['yesterday']))\n",
        "verify(cosine_similarity(e['red'],e['blue']) > cosine_similarity(e['red'],e['fast']))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verified\n",
            "Verified\n",
            "Verified\n",
            "Verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: come up with two examples that demonstrate correct similarity relations.\n",
        "verify(cosine_similarity(e['bird'],e['cat']) > cosine_similarity(e['bird'],e['computer']))\n",
        "verify(cosine_similarity(e['happy'],e['joyful']) > cosine_similarity(e['happy'],e['shoes']))"
      ],
      "metadata": {
        "id": "APqsAKrk57al",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adb0ba9e-b5fc-4540-d257-aac6c070115b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verified\n",
            "Verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: come up with two examples where cosine similarity doesn't align with your intuitions about word similarity.\n",
        "verify(cosine_similarity(e['pen'],e['paper']) > cosine_similarity(e['pen'],e['ink']))\n",
        "verify(cosine_similarity(e['car'],e['driver']) > cosine_similarity(e['car'],e['truck']))"
      ],
      "metadata": {
        "id": "-3jPqWnc58jD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c68b953b-71c6-491f-d5bc-7914cc9f1675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failure to verify\n",
            "Failure to verify\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the first example, we compared : {pen, paper} vs. {pen, ink}. While both pairs are associated with related concepts, we believe now that \"pen\" may be more commonly associated with the concept of \"writing instruments\" while \"paper\" may be more commonly associated with the concept of \"writing surfaces\". This could explain why \"pen\" and \"ink\" were found to be more similar to each other in a word embedding model than \"pen\" and \"paper\".\n",
        "\n",
        "For the second example, we compared : {car, driver} vs. {car, truck}. While both words are associated with transportation and driving, they represent different entities and concepts within that domain. Both \"car\" and \"truck\" are entities that can be used for transportation, while \"driver\" represents a different concept related to the person who operates the vehicle. We believe this could explain why \"car\" was found to be more similar to \"truck\" than to \"driver\" ."
      ],
      "metadata": {
        "id": "kPmyCqJPaUl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: extra credit goes here if you want to do it.\n",
        "def euclidean_distance(x: np.ndarray, y: np.ndarray):\n",
        "  sqrt = np.square(x - y)\n",
        "  sum = np.sum(sqrt)\n",
        "\n",
        "  return np.sqrt(sum)"
      ],
      "metadata": {
        "id": "8zVgl_2059H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we'll compare the same pair of words as we did for cosine similarity\n",
        "verify(euclidean_distance(e['apples'],e['oranges'])==euclidean_distance(e['oranges'],e['apples']))\n",
        "verify(euclidean_distance(e['car'],e['truck']) > euclidean_distance(e['car'],e['person']))\n",
        "verify(euclidean_distance(e['mars'],e['venus']) > euclidean_distance(e['mars'],e['goes'])) # TODO Convert Mars and Venus to lowercase\n",
        "verify(euclidean_distance(e['warm'],e['cool']) > euclidean_distance(e['warm'],e['yesterday']))\n",
        "verify(euclidean_distance(e['red'],e['blue']) > euclidean_distance(e['red'],e['fast']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWiNBbw_eh8l",
        "outputId": "844909cd-6509-4be9-a9ee-7e3cdd1ac45c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verified\n",
            "Failure to verify\n",
            "Failure to verify\n",
            "Failure to verify\n",
            "Failure to verify\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: come up with two examples that demonstrate correct similarity relations.\n",
        "verify(euclidean_distance(e['bird'],e['cat']) > euclidean_distance(e['bird'],e['computer']))\n",
        "verify(euclidean_distance(e['happy'],e['joyful']) > euclidean_distance(e['happy'],e['shoes']))\n",
        "\n",
        "## TODO: come up with two examples where cosine similarity doesn't align with your intuitions about word similarity.\n",
        "verify(euclidean_distance(e['pen'],e['paper']) > euclidean_distance(e['pen'],e['ink']))\n",
        "verify(euclidean_distance(e['car'],e['driver']) > euclidean_distance(e['car'],e['truck']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNfrPwTvfcd7",
        "outputId": "7c637781-063b-410c-f713-efa69c163d17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failure to verify\n",
            "Failure to verify\n",
            "Verified\n",
            "Verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observed that for every \"Verified\" pair of similarity scores computed by cosine similarity, we obtained \"Failure to verify\" pairs using Euclidean distance, and vice versa. This discrepancy can be attributed to the fundamental difference in how the two metrics capture similarities between vectors. Cosine similarity measures the similarity in direction or orientation of two vectors, whereas Euclidean distance measures the similarity in magnitude and direction of the difference between them. As a result, vectors with similar orientations but different magnitudes may be deemed similar by cosine similarity, but dissimilar by Euclidean distance, leading to opposite similarity results."
      ],
      "metadata": {
        "id": "6ifmX6BOflSU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAssYxCH4-X-"
      },
      "source": [
        "### The analogies task\n",
        "\n",
        "Given words *w1*, *w2*, and *w3*, find a word *x* such that *w1* : *w2* :: *w3* : *x*. For example, for the analogy problem *France*:*Paris* :: *England*:*x*, the answer should be *London*. To solve analogies using semantic vectors, letting $e(w)$ indicate the embedding for a word w, calculate a vector $y$ = $e(w_2)$ − $e(w_1)$ + $e(w_3)$ and find the word whose vector is closest to $y$.\n",
        "\n",
        "**TODO:** Explain why the analogy-solving method makes sense.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The analogy-solving method makes sense since it's using word embeddings that are based on the semantic relationships that are embedded in the vectors of words. Word embeddings map words to high dimensional vectors in a semantic space where similar words are located closer to each other than dissimilar ones. These spatial relationships between vectors encode semantic relationships between words.\n",
        "\n",
        "By computing a vector that captures the semantic relationship between two given words in an analogy, we can then use it to predict an unknown word related to the third word in the same way. This is done by finding the word whose vector is closest to the computed vector, using a distance metric such as cosine similarity or Euclidean distance."
      ],
      "metadata": {
        "id": "C63_xv9-gwQh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q3_OdnS3AP4"
      },
      "source": [
        "## Write a function to calculate y as described above.\n",
        "def analogy_vector(w1: str, w2: str, w3: str, e: dict) -> np.ndarray: # note that the function takes the word embedding dictionary as input.\n",
        "  return e[w2]- e[w1] + e[w3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZX3rqhb6CDG"
      },
      "source": [
        "## Write a function to find the k nearest neighbors to y.\n",
        "def analogy(w1: str, w2: str, w3: str, e: dict, k=5):\n",
        "  y= analogy_vector(w1, w2, w3, e)\n",
        "  neighbors= {}\n",
        "\n",
        "  for word in e:\n",
        "     neighbors[word] = cosine_similarity(y, e[word])\n",
        "\n",
        "  neighbors = dict(sorted(neighbors.items(), key=lambda item: item[1], reverse= True))\n",
        "  top_k_keys = []\n",
        "  for i, key in enumerate(neighbors.keys()):\n",
        "    if key!= w3:\n",
        "        top_k_keys.append(key)\n",
        "    if i == k:\n",
        "        break\n",
        "\n",
        "  return top_k_keys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Are the top 5 results for the following analogies sensible?\n",
        "print(analogy(\"france\",\"paris\",\"england\",e))\n",
        "print(analogy(\"man\",\"woman\",\"king\",e))\n",
        "print(analogy(\"tall\",\"taller\",\"warm\",e))\n",
        "print(analogy(\"tall\",\"short\",\"england\",e))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gCtuykL_Lxo",
        "outputId": "ff656f78-fe0c-46e6-f761-3aa1745632fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['london', 'manchester', 'birmingham', 'middlesex', 'liverpool']\n",
            "['queen', 'monarch', 'throne', 'princess', 'mother']\n",
            "['warmer', 'warmed', 'cooler', 'drier', 'colder']\n",
            "['short', 'following', 'wales', 'ireland', 'britain']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We belive most of the results are sensible.\n",
        "\n",
        "1) First analogy : we can see that \"london\" is the closest word to the vector \"y\" as we anticipated. In addition, all the other 4 words are also cities which makes perfect sense.\n",
        "\n",
        "2)Second analogy: the word \"queen\" is the closest to the vector \"y\" which is the correct analogy. We can see that the other 4 words are related either to words that represent  strong women, or words that are relted to monarchy which makes sense.\n",
        "\n",
        "3)Third analogy: the word \"warmer\" is the closest to the vector \"y\" which indicated that the algorithm can identify similar semantic contexts. The other 4 words that appear are similar to \"cold\" and \"warm\" and that's logical regarding the given words and context.\n",
        "\n",
        "4)Fourth analogy: we are not sure what should be the analogy here, since tall and short are opposite words but there is no opposite of a county. Thus, we belive that the fact that there are other cities and the word \"short\" in the results is reasonable. We are not sure why the word \"following\" is one of the closest words, since it has different meaning."
      ],
      "metadata": {
        "id": "wVqtme6a-R1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: come up with 4 more analogies, 2 of which work in your opinion, and 2 of which don't work.\n",
        "#analogies which we believe work\n",
        "print(analogy(\"small\",\"smaller\",\"big\",e))\n",
        "print(analogy(\"puppy\",\"dog\",\"kitten\",e))\n",
        "\n",
        "#analogies which we believe won't work\n",
        "print(analogy(\"lemon\",\"sour\",\"sugar\",e))\n",
        "print(analogy(\"guitar\",\"music\",\"brush\",e))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYMdmPFNvV3n",
        "outputId": "573f817d-2a35-4310-e656-59421d2c8083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['bigger', 'smaller', 'larger', 'biggest', 'much']\n",
            "['dog', 'cat', 'dogs', 'pet', 'cats']\n",
            "['sour', 'cocoa', 'milk', 'corn', 'dairy']\n",
            "['scrub', 'dry', 'music', 'aside', 'brushes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__P1YzLM6ixs"
      },
      "source": [
        "**TODO:**\n",
        "Did you notice any patterns or generalizations while exploring possible analogies? For the ones that went wrong, why do you think they went wrong?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We believe that in order to obtain accurate results, the analogies should be precise between the words. Furthermore, the dataset includes plural, comparative, and superlative forms of certain words, which we think can pose a challenge for the algorithm to accurately predict the appropriate contexts and analogies.\n",
        "\n",
        "For example:\n",
        "\n",
        "1) Paris is **the capital of** France, just as London is **the capital of** England.\n",
        "\n",
        "2) \"smaller\" is the comparative form of the adjective \"small\", just as \"big\" and \"bigger\". Similar to the given example of \"tall\":\"taller\" :: \"warm\": \"warmer\".\n",
        "We can see that the algorithm is able to identify this words correctly.\n",
        "\n",
        "For the examples that didn't work, the relation between the words is a bit more complex:\n",
        "\n",
        "1) guitar is an **instrument used to make music** , while a brush is a **tool used to apply paint**. We believe that if we have tried a different type of musical instrument, we would have get better results.\n",
        "\n",
        "2) We believed that since puppy **is a young** dog and kitten **is a young** cat, we would get the correct analogy. However, the closest word we got was \"dog\", rather than \"cat\". Additionally, the results also included the plurals of \"dog\" and \"cat\", which supports our previous conclusion.\n",
        "\n",
        "The algorithm struggled to identify more complex analogies, even though the failed examples seemed logical and understandable to human thinking."
      ],
      "metadata": {
        "id": "BiXzIj6-4AjB"
      }
    }
  ]
}